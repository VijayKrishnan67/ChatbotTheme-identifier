{
  "doc_id": "DOC004",
  "filename": "A_Spatio-_Temporl_Deepfake_Video_Detection_Method_Based_on_TimeSformer-CNN (1).pdf",
  "extraction_method": "pdf-text",
  "upload_time": "2025-06-03T05:47:52Z",
  "content": [
    "2024 Third International Conference on Distributed Computing and Electrical Circuits and Electronics (ICDCECE) \n979-8-3503-1860-9/24/$31.00 ©2024 IEEE \nA Spatio-Temporl Deepfake Video Detection \nMethod Based on TimeSformer-CNN \n1st Zhengxuan Chen \nInstitute of Information and Network Security \nPeople's Public Security University of China \nBeijing, China \nchanchenghin69@gmail.com \n2nd Shuo Wang \nInstitute of Criminal Investigation \nPeople's Public Security University of China \nBeijing, China \n202121210034@stu.ppsuc.edu.cn \n \n3rd Deyang Yan \nInstitute of National Security \nPeople's Public Security University of China \nBeijing, China \n202121210039@stu.ppsuc.edu.cn \n \n4th Yushi Li \nInstitute of Public Economics and Administration  \nShanghai University of Finance and Economics \nShanghai, China \nlieeuunniiccee@gmail.com \n \nAbstract—In order to improve the accuracy of real-time \ndeepFake video detection, this paper proposes a method based \non the EfficientNet-TimeSformer model. Firstly, we use the \ntransfer Xi method to use a pre-trained convolutional network \nfor spatial feature extraction, and introduce a lightweight \nnetwork for exploration, with an accuracy of 95.26% and an \nAUC of 95.52, which is very impressive. At the same time, the \ntrainable parameters of the model are only 5,234,124, which is \nsignificantly higher than that of other models. Then, we optimize \nthe model through data augmentation such as pre-training and \ndataset sizing to accommodate larger image sizes and longer \nvideo input sequences, so as to enhance the model's ability to \nprocess complex video data. The empirical results show that our \nmodel performs well in high-resolution videos and longer video \nsequences, significantly outperforming the performance of \ntraditional convolutional neural networks, taking into account \nGPU memory limitations. At the same time, some limitations in \nthe study are noted, such as the failure to test video clips longer \nthan 96 frames and some limitations on the Celeb-DF dataset. \nFuture research directions include in-depth research on the \ninterpretability and generalizability of the model, as well as \nverifying its performance in a wider range of application \nscenarios.  \nKeywords—deepfake, \nreal-time \ndetection, \nlightweight, \nefficientnet-timesformer network \nI. INTRODUCTION \nIn recent years, the rapid development of deep learning Xi \ntechnology, especially the development of generative \nadversarial networks and variational autoencoders, has made \ndeep pseudo technology increasingly mature and realistic. \nDue to their huge deceptiveness, criminals have used them to \nfake face videos, \nIt is used for illegal and criminal activities such as fake \nmedia, fraud, and telecommunication fraud. At present, the \nmethods for detecting deep fake videos are mainly divided \ninto two categories, which are space-based and space-time-\nbased. The spatial-based deep fake video detection method \nfocuses on the analysis of a single frame of the image, aiming \nto find out whether the image has been tampered with. This \nmethod uses techniques such as convolutional neural \nnetworks (CNNs) to extract the spatial features of the image, \nand through in-depth analysis of these features, determine \nwhether the image is a deepfake. In contrast, the \nspatiotemporal-based deep pseudo video detection method \nconsiders the temporal information of the entire video \nsequence, rather than being limited to a single frame. This \nmethod not only makes use of spatial information, but also \nfully considers the temporal evolution between frames to \ndetect tampering in videos more comprehensively. Typical \nstructures include convolutional neural networks (CNNs) and \nlong \nshort-term \nmemory \nnetworks \n(LSTMs) \nto \nsimultaneously capture spatiotemporal information.  \nThe purpose of this paper is to propose a deep pseudo \nvideo detection method with low computing and low memory \nrequirements, using the EfficientNet architecture to detect in \nspace and TimeSformer to detect in time, so as to expand the \napplication scenarios of deep pseudo detection technology in \nreal life. \nII. RELATED WORK \nIn recent years, with the continuous development of deep \nXi technology, the abuse of deepfake technology has gradually \nattracted widespread attention. In order to effectively address \nthis challenge, researchers have carried out in-depth research \nin different directions. \nIn a 2020 study [1], Tarasiou et al. proposed a lightweight \ndeep Xi architecture called \"Audio-Visual Person-of-Interest \nDeepFake Detection\" to enable efficient detection of fully \ngenerated facial images by sharing local image features across \nmanipulated regions. The study published by Coccomini et al. \nin 2022 [2] combined EfficientNet B0 convolution and \ndifferent types of Vision Transformers to achieve efficient \nvideo deepfake detection through a simple voting scheme, \nwhich performed remarkably. The 2021 study by Wodajo et \nal. [3] explored a convolutional visual Transformer that \ncombined a convolutional neural network (CNN) and a visual \nTransformer (ViT) to achieve competitive detection results on \nthe DFDC dataset. The study proposed by Pintelas et al. [4] in \n2022 introduced an unsupervised CNN topology and \ncombined 3D-CAE and 3D-CNN to construct a deep \nrepresentation Xi framework, making full use of the \nadvantages of unsupervised and supervised methods in 3D \nimage input. The study by Zhao et al. (2023) [5] introduced an \ninterpretable spatiotemporal video transformer, including a \nnovel mechanism for decomposing spatiotemporal self-\nattention and self-reduction to enable robust deepfake \ndetection. Yin et al. (2023) studied interframe motion over \nlong and short distances by [6] and proposed a dynamic \ndifference Xi method for modeling accurate spatiotemporal \ninconsistencies. Ismail et al. (2022) achieved higher \nperformance by combining two feature extraction methods \nusing the YOLO facial detector and the improved HOG-based \nXceptionNet CNN through a study by [7]. The [8] proposed \nby Saikia et al. (2022) uses an optical flow-based feature \n2024 Third International Conference on Distributed Computing and Electrical Circuits and Electronics (ICDCECE) | 979-8-3503-1860-9/24/$31.00 ©2024 IEEE | DOI: 10.1109/ICDCECE60827.2024.10549278\nrized licensed use limited to: AMRITA VISHWA VIDYAPEETHAM AMRITA SCHOOL OF ENGINEERING. Downloaded on September 13,2024 at 13:08:16 UTC from IEEE Xplore.  Restrictions a\n",
    "extraction method to achieve earlier deepfake detection \nthrough a hybrid model. The study by De Lima et al. (2020) \n[9] evaluated the effect of the action recognition method in \ndeepfake detection by testing different networks, and pointed \nout that the R3D model based on ResNet had the highest \nperformance. Some lightweight deep learning Xi models have \nalso made significant progress in the field of deepfake \ndetection. Fang et al. (2022) [10] implemented DeepFake \nvideo detection using lightweight CNNs with sparse optical \nflow on the face, demonstrating a balance of high accuracy \nand low computational cost.  \nThese studies provide abundant ideas and methods for the \nfield of deepfake detection, and provide a useful reference for \nfurther improving the detection accuracy and robustness. In \nthe future, as technology evolves, we can expect more \ninnovative ways to emerge to better protect society from the \nthreat of deepfakes. \nIII. RELATED MODELS \nThis article describes a faster, lighter model, as well as the \ndatasets used and the novel augmentation strategies applied in \nthis work. We mainly propose a new lightweight model based \non EfficientNet architecture and TimeSformer network, which \ncan learn from the features of spatial and temporal domains \nand Xi to achieve deep pseudo-video detection with low \ncomputing and low memory requirements. \nA. Dataset \nIn the course of this study, we made full use of the Celeb-\nDF (Celebrities DeepFake) dataset, a large-scale dataset \nfocused on the study of deepfake videos of celebrities. The \ndataset consists of facial images of real celebrities and \ndeepfake videos generated from those images. Among them, \ndeepfake videos use advanced generative adversarial network \n(GAN) technology to mimic the real facial expressions and \nmovements of celebrities. Celeb-DF is unique in its hybrid \nsamples, which combine real facial images of celebrities with \nfake videos, thus providing a more difficult challenge for \nmodels to more accurately capture the subtle facial features of \ncelebrities. In addition, the Celeb-DF dataset also provides \ngeolocation information for each video, providing researchers \nwith deeper contextual information that helps to understand \nthe source of the video. This makes Celeb-DF a challenging \nand practical dataset for the study of deepfake detection \nalgorithms. \nA major problem in the Celeb-DF dataset is the category \nimbalance, which refers to the uneven distribution of sample \nsizes of real and deepfake videos in the dataset. This class \nimbalance can negatively impact the training and performance \nof deep learning Xi models. During training, the model may \nbe more inclined to learn the features of Xi real video, as \nsamples in this category are more prominent. This can lead to \na decrease in the model's performance on deepfake videos \nbecause it doesn't pay enough attention to this relatively small \ncategory. \nTo address the category imbalance, we employ a variety of \ndata augmentation techniques. By randomly clipping, \nhorizontal flipping, rotation, brightness, contrast, and \nsaturation adjustments, adding noise, blending, and other \noperations, we introduce diversity into the training data and \nimprove the generalization ability of the model. To merge \nmultiple video sources into a single scene, we created a 15-\nsecond long video with 3 seconds per clip. Select five random \nindexes through a random number generator and connect five \nrandom 3-second long videos, making sure these videos are \nfrom the real video category and there is no mixing of \nDeepFake and real videos. The ingenuity of this enhancement \nmethod is its ability to generate up to 7.14*10^13 unique \nvideo combinations. In this study, we generated a total of 5639 \nvideos, including real and fake videos. This strategy not only \naddresses the category imbalance in the Celeb-DF dataset \nfrom 90:10 to 50:50, but also covers key scenarios to ensure \nthat the video contains a variety of different content. \nEventually, we divided the data into training and test sets \naccording to the standard 80:20 ratio, with 80% for training \nand 20% for testing, to ensure that the model was effectively \ntrained and evaluated on different subsets of data. \nB. EfficientNet \nDeep-learning Xi models need to process large amounts of \ndata quickly and accurately in real-time deepFake video \nscenes, and EfficientNet achieves excellent performance with \na relatively small number of parameters through a composite \nscaling strategy. This is crucial for real-time video detection, \nbecause it not only improves the computational efficiency of \nthe model, but also better captures the subtle spatiotemporal \nfeatures in the video through a reasonable balance of network \ndepth, width, and resolution, thereby enhancing the sensitivity \nto synthetic traces and subtle movements. We use the \nEfficicientNet-B0 network as the backbone of the spatial \nfeature extraction of video frames, and pre-train it on \nImageNet, which greatly reduces the training parameters and \nmakes it more suitable for running in resource-constrained \nenvironments. The pre-trained EfficicientNet-B0 network \nprovides an ideal solution for real-time performance and \naccuracy requirements, and becomes a powerful tool to \naddress the challenges of real-time deepFake video detection, \nas shown in Fig.1. \n \nFig. 1. Schematic diagram of the network framework of EfficientNet-B0 \nThe network framework of EfficientNet-B0 is divided into \n9 stages, as shown in Table I. \nTABLE I.  \nCOMPARISON OF ACCURACY OF DIFFERENT MODELS \nStage \nOperator \nResolution \nChannels \nLayers \n1 \nConv3×3 \n224×224 \n32 \n1 \n2 \nMBConv1, k3×3 \n112×112 \n16 \n1 \n3 \nMBConv6, k3×3 \n112×112 \n24 \n2 \n4 \nMBConv6, k5×5 \n56×56 \n40 \n2 \n5 \nMBConv6, k3×3 \n28×28 \n80 \n3 \n6 \nMBConv6, k5×5 \n14×14 \n112 \n3 \n7 \nMBConv6, k5×5 \n14×14 \n192 \n4 \n8 \nMBConv6, k3×3 \n7×7 \n320 \n1 \nrized licensed use limited to: AMRITA VISHWA VIDYAPEETHAM AMRITA SCHOOL OF ENGINEERING. Downloaded on September 13,2024 at 13:08:16 UTC from IEEE Xplore.  Restrictions a\n",
    "9 \nConv1\n×\n1&Pooling&FC \n7×7 \n1280 \n1 \nThe first Stage is a common convolutional layer with a \nconvolution kernel size of 3x3 and a step of 2 (including BN \nand the activation function Swish), Stage2~Stage8 are all \nstacking the MBConv structure repeatedly (the Layers in the \nlast column indicate how many times the stage repeats the \nMBConv structure), and Stage9 is composed of an ordinary \n1x1 convolutional layer (including BN and the activation \nfunction Swish), an average pooling layer and a fully \nconnected layer. Each MBConv in the table is followed by a \nnumber 1 or 6, where 1 or 6 is the magnification factor n, that \nis, the first 1x1 convolutional layer in MBConv will expand \nthe channels of the input feature matrix to n times, where k3x3 \nor k5x5 represents the size of the convolution kernel used by \nDepthwise Conv in MBConv. Channels indicates the channels \nthat output the feature matrix after passing through the stage. \nC. TimeSformer \nTime domain modeling of video sequences is crucial in \ndeep learning Xi, as DeepFake often involves subtle temporal \ncorrelation and subtle facial movements. By introducing a \nspatiotemporal attention mechanism, TimeSformer is able to \ncapture key moments in video sequences more flexibly, which \nhelps to identify subtle changes in deepfake synthesis. This \nattention mechanism allows the network to focus more on the \ntime segments in the video sequence that are crucial for \nauthenticity judgment, thus increasing the sensitivity to \nsynthetic video. Since real-time deepfake video detection \nrequires timely and accurate feedback, TimeSformer's time-\ndomain attention mechanism makes it better than traditional \nnetworks in capturing video dynamic features, providing a \nmore powerful tool for real-time application scenarios. The \nspecific process is as follows: \nStep1:  Enter Video Clip: Suppose we have a video clip \nof length T which contains T consecutive video frames, \nand the features of each frame are represented as, where \nH, W, and C represent the height, width, and number of \nchannels of the image, respectively. \u0001\u0002 ∈ℝ\u0005\u0006\u0007\u0006\b,  as \nshown in Fig.2 \n \nFig. 2. The input video clip is made up of a series of picture frames \nStep2: Image Block Splitting: Split the image block for \neach video frame Xt and split it into M×N image \nblocks, as shown in Fig.3. The size of  each image \nblock is H  block×Wblock×C, where Hblock and \nWblock represent the height and width of each image \nblock, respectively.  \n \nFig. 3. Slice the image frame into image blocks \nStep3: Linear Embedding: Linear embedding is \nperformed in order to map the features of each video \nframe to a higher-dimensional space, as shown in Fig.4. \nThis can be represented by the matrix Wembed: \n\t\u0002 \n \u0001\u0002 ⋅\f\r\u000e\u000f\r\u0010 \u0011 \u0012\r\u000e\u000f\r\u0010                     (1) \nwhere D represents the embedded dimension.\t\u0002 ∈ℝ\u0013\u0006\u0014 \n \nFig. 4. Linear embedding layer \nStep4: QKV Calculation (Query, Key, Value): The \nresults of linear embedding are used to calculate \nQuery(Q), Key(K), Value(V)) , as shown in Fig.5. \n\u0015\u0016\u0017,\u0002\u0019\n\u0016\u001a,\u001b\u0019 \n \f\u001c\n\u0016\u001a,\u001b\u0019\u001d\u001e\u0016\u001f\u0016\u0017,\u0002\u0019\n\u0016\u001a !\u0019\u0019 ∈ℝ\u0014\"\n#\u0016\u0017,\u0002\u0019\n\u0016\u001a,\u001b\u0019 \n \f$\n\u0016\u001a,\u001b\u0019\u001d\u001e\u0016\u001f\u0016\u0017,\u0002\u0019\n\u0016\u001a !\u0019\u0019 ∈ℝ\u0014\"\n%\u0016\u0017,\u0002\u0019\n\u0016\u001a,\u001b\u0019 \n \f&\n\u0016\u001a,\u001b\u0019\u001d\u001e\u0016\u001f\u0016\u0017,\u0002\u0019\n\u0016\u001a !\u0019\u0019 ∈ℝ\u0014\"\n              (2) \n where WQ, WK, and WV are the corresponding weight \nmatrices.  \n \nFig. 5. QKV calculations \nStep5: Self-attention calculation: Use Query, Key, and \nValue to calculate the self-attention weight matrix At: \nrized licensed use limited to: AMRITA VISHWA VIDYAPEETHAM AMRITA SCHOOL OF ENGINEERING. Downloaded on September 13,2024 at 13:08:16 UTC from IEEE Xplore.  Restrictions a\n",
    "'\u0016\u0017,\u0002\u0019\n\u0016\u001a,\u001b\u0019\u0002(\u000e\r \n )*\u0016\n+\u0016,,-\u0019\n\u0016.,/\u0019\n0\u0014\" ⋅[#\u00162,2\u0019\n\u0016\u001a,\u001b\u0019{#\u0016\u0017,\u00024\u0019\n\u0016\u001a,\u001b\u0019 }\u000246!,...,8]\u0019    (3) \nThen, apply these attention weights to the value matrix \n\\(V_t\\) to get the output of self-attention, as shown in Fig.6: \n\u001f\u0016\u0017,\u0002\u0019\n\u0016\u001a\u0019\n\n *\u001d:\u0016\u001d\u001e\u0016\u001f\u0016\u0017,\u0002\u0019\n 4\u0016\u001a\u0019 \u0019\u0019 \u0011 \u001f\u0016\u0017,\u0002\u0019\n 4\u0016\u001a\u0019         (4) \n \nFig. 6. Self-attention module \nStep6: Classification: Pooling or integrating the results \nof self-attention calculation, and then performing the \nfinal classification through the fully connected layer \n< \n *\u001d:\u0016\u001f\u00162,2\u0019\n\u0016=\u0019 \u0019                               (5) \nD. Efficientnet-TimeSformer network \nEfficientNet-TimeSformer Lightweight Network is a deep \nXi architecture that combines EfficientNet and TimeSformer \nto achieve lightweight and efficient time-domain modeling of \nvideo sequences. The network combines the parameter \nefficiency and lightweight design of EfficientNet with the \ntime-domain attention mechanism of TimeSformer, making it \nexcellent for tasks such as real-time video processing and \ndeepfake detection. \nFirst, the video frames in the input video sequence are \nbroken down into image blocks to accommodate the video \ninput of different sizes and resolutions. The pre-trained \nEfficientNet model then processes each image block, \nincluding convolutional neural network (CNN) layers and \nbalanced adjustment of depth, width, and resolution to extract \nspatial features and reduce computational burden. \nNext, the time-domain modeling of TimeSformer is \nadopted. Firstly, the frame feature sequence processed by \nEfficientNet is converted or reshaped as necessary to adapt to \nthe input format of TimeSformer. Then, the temporal \ninformation in the video sequence was captured through linear \nembedding, QKV calculation and self-attention mechanism. \nThis helps the network better understand the dynamic changes \nand temporal correlations in the video, and increases the \ndetection sensitivity of deepfake videos. \nFinally, through the integration of mixed attention \nprocessing and spatiotemporal features, the final prediction \nresults are generated by classification through pooling and \nfully connected layers. Due to the lightweight design of the \nEfficientNet-TimeSformer network, it reduces the number of \nparameters and computational cost of the model while \nmaintaining efficient performance, making it especially \nsuitable for resource-constrained environments and scenarios \nthat require high real-time performance. The specific process \nis shown in the following Fig.7: \n \nFig. 7. Schematic diagram of the Efficientnet-TimeSformer network flow \nIV. RESULTS AND DISCUSSION \nThe experiment was conducted on a server equipped with \nan NVIDIA GeForce GTX 1080 Ti GPU, and the model was \ntrained using the TensorFlow framework. The Celeb-DF \ndataset was selected as the experimental data, containing both \nreal and fake videos, using 15-second long video clips as \ninput. \nIn this study, Accuracy and AUC were used as evaluation \nindicators to comprehensively evaluate the performance of the \nmodel. As shown in Table II, the proposed model outperforms \nthe previous model in terms of both accuracy and AUC. The \naccuracy of the proposed Efficientnet-TimeSformer network \nis 95.26%, and the AUC is 95.52. \nTABLE II.  \nCOMPARISON OF ACCURACY OF DIFFERENT MODELS \nModel \nAccuracy \nAUC \n2D-CNN \n92.62% \n \n3D-CNN \n98% \n99.70 \nHRNe \nOne \n74.76 \nR3D \n98.26% \n99.73 \nCNN (inspired by Heart rate \nestimator \n98.7% \n \nFacial Feature Extraction \nOne \n97.00 \nHybrid CNN-LSTM with Optical \nFlow \n79.49% \n79.00 \nResnext and LSTM \n— \n88.80 \nEfficientnet-TimeSformer \n95.26% \n95.52 \n \nAlthough the proposed model does not have an advantage \nin accuracy compared with traditional recognition models, \nthrough the optimization of EfficientNet and the careful \nadjustment of the network structure, we have successfully \nreduced the number of trainable parameters of the model and \nreduced the computational complexity while maintaining \nperformance. At the same time, the time-domain modeling \nmechanism of TimeSformer is introduced, which makes the \nrized licensed use limited to: AMRITA VISHWA VIDYAPEETHAM AMRITA SCHOOL OF ENGINEERING. Downloaded on September 13,2024 at 13:08:16 UTC from IEEE Xplore.  Restrictions a\n",
    "model more effective in capturing the temporal information in \nthe video sequence, thereby improving the performance in the \nreal-time deepFake video detection task. This lightweight \ndesign not only reduces resource requirements, but also makes \nit more suitable for a variety of computing environments. By \nemploying the data augmentation method of multi-source \nvideo merging, we further improve the robustness and \ngeneralization ability of the model, making it an excellent \nsolution for lightweight tasks, as shown in Table III. \nTABLE III.  \nCOMPARATIVE ANALYSIS OF TRAINABLE \nPARAMETERS FOR THE PROPOSED MODEL AGAINST VARIOUS STATE-OF-\nTHE-ART MODELS ON CELEB DF DATASET \nModel \nTminable \nparameter \nRatio of accuracy to \nparameter (in million) \n2D-CNN \n1,000,000 \n92.62 \n3D-CNN \n>55,000,000 \n1.78 \nHRNet \nOne \nOne \nR3D \n33,170,000 \n2.96 \nCNN (inspired by Heart \nrate estimator) \n1,000,000 \n98.70 \nFacial Feature Extraction \n1,600,000 \n \nEfficientnet-\nTimeSformer \n5,234,124 \n2,452.90 \n \nIn order to improve the accuracy of the model, we try to \nchange some of the parameters or modules for comparison. \nIncreasing the size of the image is often seen as an effective \nmeans of improving the accuracy of the model. This is because \nlarger images contain more information and detail, allowing \nthe model to look at the image context and microstructure \nmore comprehensively. High-resolution images help models \ncapture objects and patterns in images in greater detail, \nespecially when complex visual tasks such as object \nrecognition and fine-grained classification are required. When \nthe size of each image tile remains the same, the number of \nimage patches increases as the image size increases. At the \nsame time, as the number of frames increases, so does the \namount of data processed by the input attention mechanism. It \ncan be clearly seen from the graph that with the increase of the \nrichness of input information, the performance of the \nEfficientnet-TimeSformer network is greatly improved, as \nshown in Fig.8. \n \n \nFig. 8. Comparison of accuracy results for different image sizes \nThe pre-training phase enhances the generalization ability \nof the model by learning Xi common features on large-scale \ndata, providing the model with a deep understanding of \ncomplex data. Adjusting the size of the dataset, especially \nexpanding the size of the dataset, can provide more rich and \ndiverse samples for the model, so that it can learn the \ndistribution and characteristics Xi of the data more \ncomprehensively, and then improve the performance in \npractical tasks. To investigate the impact of the size of the \ndataset, four groups were used using 25%, 50%, 75% and \n100% data. The result is that the EfficientNet-TimeSformer \nnetwork does not perform well when there is less data and \nbetter when there is more data, as shown in Fig.9. \n \nFig. 9. Compare the accuracy of different dataset sizes \nV. CONCLUSION \nIn this study, we aim to improve the accuracy of real-time \nDeepFake video detection, and we introduce a lightweight \nEfficientNet-TimeSformer network and optimize model \nperformance through pre-training and dataset size adjustment. \nOur empirical research shows that increasing the image size \nand longer video input sequences have a significant impact on \nthe improvement of model accuracy. \nHowever, there are still some limitations to our study. \nFirst, due to GPU memory limitations, we were unable to test \nvideo clips beyond 96 frames, so the model's performance at \nlonger videos was not fully validated. Second, although the \ndataset used in our experiment is a standard Celeb-DF, there \nmay still be some deviations in some specific scenarios. \nFuture studies can further validate the robustness of the model \nby using larger, more diverse datasets. \nIn future research, we plan to expand the study of model \ninterpretability to better understand its decision-making \nprocess. In addition, we will explore a wider range of \napplication scenarios, such as online video stream monitoring, \nto verify the robustness of the model in real-world \napplications. We also plan to conduct a more comprehensive \nevaluation of the generalizability and generalization \nperformance of the model. Overall, although this study has \nmade significant progress, more work is still needed to further \nimprove and advance the research in the field of real-time \ndeepfake video detection. \nAUTHOR CONTRIBUTIONS \nZhengxuan Chen and Shuo Wang are co-first authors. \nThe corresponding author is Shuo Wang. \nREFERENCES \nrized licensed use limited to: AMRITA VISHWA VIDYAPEETHAM AMRITA SCHOOL OF ENGINEERING. Downloaded on September 13,2024 at 13:08:16 UTC from IEEE Xplore.  Restrictions a\n",
    "[1] Tarasiou, M., & Zafeiriou, S. (2020). Extracting deep local features to \ndetect manipulated images of human faces. In *2020 IEEE \ninternational conference on image processing (ICIP)* (pp. 1821-1825). \nIEEE. \n[2] Coccomini, D. A., Messina, N., Gennaro, C., et al. (2022). Combining \nefficientnet and vision transformers for video deepfake detection. In \n*International conference on image analysis and processing* (pp. 219-\n229). Springer International Publishing. \n[3] Wodajo, D., & Atnafu, S. (2021). Deepfake video detection using \nconvolutional vision transformer. *arXiv preprint arXiv:2102.11126*. \n[4] Pintelas, E., & Pintelas, P. (2022). A 3D-CAE-CNN model for Deep \nRepresentation Learning of 3D images. *Engineering Applications of \nArtificial Intelligence*, 113, 104978. \n[5] Patil S, Balmuri KR, Frnda J, Parameshachari BD, Konda S, Nedoma \nJ. Identification of Triple Negative Breast Cancer Genes Using Rough \nSet Based Feature Selection Algorithm & Ensemble Classifier. \nHUMAN-CENTRIC \nCOMPUTING \nAND \nINFORMATION \nSCIENCES. 2022 Nov 30;12. \n[6] Yin, Q., Lu, W., Li, B., et al. (2023). Dynamic Difference Learning \nwith Spatio-temporal Correlation for Deepfake Video Detection. \n*IEEE Transactions on Information Forensics and Security*, 2023. \n[7] Ismail, A., Elpeltagy, M., Zaki, M. S., et al. (2022). An integrated \nspatiotemporal-based methodology for deepfake detection. *Neural \nComputing and Applications*, 34(24), 21777-21791. \n[8] Bandani AK, Riyazuddien S, Bidare Divakarachari P, Patil SN, Arvind \nKumar G. Multiplicative long short-term memory-based software-\ndefined networking for handover management in 5G network. Signal, \nImage and Video Processing. 2023 Sep;17(6):2933-41. \n[9] Lima, O., Franklin, S., Basu, S., et al. (2020). Deepfake detection using \nspatiotemporal \nconvolutional \nnetworks. \n*arXiv \npreprint \narXiv:2006.14749*. \n[10] Damalla R, Datla R, Vishnu C, Mohan CK. Self-supervised embedding \nfor generalized zero-shot learning in remote sensing scene \nclassification. Journal of Applied Remote Sensing. 2023 Jul \n1;17(3):032405-. \nrized licensed use limited to: AMRITA VISHWA VIDYAPEETHAM AMRITA SCHOOL OF ENGINEERING. Downloaded on September 13,2024 at 13:08:16 UTC from IEEE Xplore.  Restrictions a\n"
  ]
}