{
  "doc_id": "DOC002",
  "filename": "s11227-024-06602-y.pdf",
  "extraction_method": "pdf-text",
  "upload_time": "2025-06-01T09:16:31Z",
  "content": [
    "Vol.:(0123456789)\nThe Journal of Supercomputing           (2025) 81:28 \nhttps://doi.org/10.1007/s11227-024-06602-y\nResearch on multimodal hate speech detection based \non self‑attention mechanism feature fusion\nJunjie Mao1 · Hanxiao Shi1 · Xiaojun Li1\nAccepted: 7 October 2024 \n© The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature \n2024\nAbstract\nThe widespread rise of multimedia social platforms has diversified the ways in \nwhich people communicate and the content they share. Hate speech, as a threat to \nsocietal harmony, has also shifted its manifestation from a singular textual to a mul-\ntimodal one. Previously, most methods for detecting hate speech were limited to the \ntext modality, making it difficult to identify and classify newly emerging multimodal \nhate speech that combines text and images. This paper proposes a novel multimodal \nhate speech detection model to respond to the above-mentioned needs for multi-\nmodal hate speech detection. The proposed joint model can use moving windows to \nextract multi-level visual features and extract text features based on the RoBERTa \npretraining model and introduces a multi-head self-attention mechanism in the later \nfusion process for image and text feature fusion. This article also conducted experi-\nments on the multimodal benchmark dataset hateful memes. The model achieved \nan accuracy of 0.8780, precision of 0.9135, F1-score of 0.8237, and AUCROC of \n0.8532, defeating the SOTA multimodal hate speech recognition model.\nKeywords  Hateful speech detecting · Deep learning · Multimodal data analysis · \nModal fusion · Attention mechanism\n *\t Hanxiao Shi \n\t\nhxshory@foxmail.com\n\t\nJunjie Mao \n\t\n1835020317@pop.zigsu.edu.cn\n\t\nXiaojun Li \n\t\nlixj@zjsu.edu.cn\n1\t\nSchool of Management and E‑Business, Zhejiang Gongshang University, Hangzhou 310018, \nZhejiang, China\n",
    "\t\nJ. Mao et al.\n   28  \nPage 2 of 17\n1  Introduction\nWith the growing role of social media and online communities in our daily \nlife, monitoring and managing hate speech has become particularly important. \nBecause it not only expands the channels for people to exchange information \nand facilitates the way people communicate, but also induces the breeding of \nhate speech due to the anonymity and freedom of online speech [1]. The United \nNations defines hate speech as any kind of communication in speech, writing, or \nbehavior, that attacks or uses pejorative or discriminatory language with refer-\nence to a person or a group on basis of who they are, in other words, based on \ntheir religion, ethnicity, nationality, race, color, descent, gender, or other identity \nfacto [2]. To some extent, hate speech can lead to social conflicts and vicious \nevents, and even endanger the harmonious development of society. For example, \nthe second study on hate speech and discrimination in Costa Rica in 2022 found \nthat hate speech caused division and a bad atmosphere in Costa Rica [3]. There-\nfore, the detection of hate speech has become a key social issue and is regarded as \na very important task with potential significant benefits in the research field.\nIn multimodal scenarios, hate speech detection (HSD) is facing great chal-\nlenges. Due to the huge amount of data involved, traditional manual detection \nmethods are inefficient [4, 5]. With the development of natural language pro-\ncessing and machine learning technology, HSD has been able to automatically \nidentify hate speech in text [6]. However, the development of multimedia makes \npeople’s communication mode not only limited to text, but also more willing to \nuse multimodal superimposed information such as pictures, videos, or text as the \ncarrier of communication. In this process, hate speech has become more difficult \nto detect due to the diversification of information carriers. As shown in Fig. 1, \na certain image may not have hate factors by itself, but it can produce different \nemotional tendencies when paired with a certain sentence. The example in Fig. 1, \nafter text is paired with images, can express gender discrimination, insult, por-\nnography, and other hateful connotations. The text information in the first image \nreflects anti-lesbian remarks, the second image implies support for drugs, espe-\ncially from the needle and other signs in the image, the text in the third image \ncombined with the image implies racial discrimination, and the fourth image \ncompares the hanging of the head to a swing, containing meanings such as vio-\nlent death.\nMultimodal hate speech detection (HSD) is defined as a technology that uses \nmultimodal information including images and text to capture and distinguish hate \nspeech [7]. Compared with a single text analysis method, this method can more \naccurately and efficiently realize the automation of hate speech detection.\nThis paper aims to explore the methods and applications of multimodal hate \nspeech detection and construct a multimodal hate speech detection model based \non image–text fusion to improve the accuracy and robustness of hate speech \ndetection. In this paper, hateful memes, the currently typical multimodal hate \nspeech dataset, will be used for the training and validation of the model.\nThe main contributions of our work are summarized as follows:\n",
    "Research on multimodal hate speech detection based on…\nPage 3 of 17 \n   28 \n1.\t This paper proposes a new joint model for multimodal hate speech detection, \nwhich uses a moving window for multi-level visual feature extraction for visual \nmodalities and a RoBERTa pretrained model for textual modalities. In addition, \nthe structure of the model is relatively clear and highly interpretable.\n2.\t This paper innovatively introduces a multi-head self-attention mechanism at the \nmodel fusion stage. This mechanism allows the model to dynamically adjust the \nweight of text features, effectively fusing image, and text information. Important \ntext parts better match image features better, thereby improving the fusion effect.\n3.\t This paper carries out a series of experiments on the characteristics, content, and \nmodes of the benchmark dataset hateful memes for multimodal hate speech. We \nseparate the text and image in the memes, removing the text to avoid interference \nwith image feature extraction. Using innovative joint methods, this paper achieves \nbetter results on hateful memes than the SOTA model.\n2  \u0007Related work\nThis section reviews the literature on the hate speech detection, considering both single \ntext and multimodal aspects.\nFig. 1   Examples of hateful memes\n",
    "\t\nJ. Mao et al.\n   28  \nPage 4 of 17\n2.1  \u0007Text hate speech detection\nHere is a large body of relevant literature on hate speech detection. As early as the \nbeginning of 2010, people had begun to study hate speech detection. Most of the \nexisting research on hate speech detection focuses on hate speech content related to \nimmigration or gender discrimination issues [8–10]. In addition, hate speech such as \nracial discrimination is also the focus of research [11, 12]. Flor Miriam Plaza-del-\nArco et al. compared the Spanish hate speech detection pretraining language model \nbased on the transformer mechanism [13].\nEarly hate speech detection was mainly based on text feature engineering, using \ntechniques such as TF-IDF and n-gram to represent the textual characteristics of \nhate speech, and on this basis, combining machine learning algorithms such as sup-\nport vector machines, naive Bayes, and logistic regression to perform classification \nand detection of hate speech text [14]. In addition, dictionary-based methods mainly \nuse fixed or existing keywords (dictionaries) extracted from relevant hate texts for \nhate speech detection [15]. However, such methods are often limited in feature \nrepresentation and model generalization. The rise of deep learning technology has \nbrought new possibilities for feature representation and model generalization. Con-\nvolutional neural network (CNN) is also commonly used for text classification [16]. \nBjörn Gambäck and Utpal Kumar Sikdar trained four convolutional neural network \nmodels, respectively, and the word2vec embeddings performed the best with an \nF1-score of 78.3% [17]. Moreover, the recurrent neural network (RNN) and trans-\nformer model can take context information into account when processing text data, \nto better capture semantics and context. Alshalan and others compared three net-\nwork architectures of CNN, GRU, and CNN + GRU, and the model effect of BERT \nwas evaluated [18]. It was found that the CNN model had the best performance, with \nan F1-score of 0.79.\n2.2  \u0007Multimodal hate speech detection\nModel fusion is a technique that combines multiple machine learning or deep learn-\ning models to improve the performance of the model. Such methods can be applied \nacross various deep learning domains, including but not limited to computer vision, \nnatural language processing, and speech recognition. The fusion of deep learning \ncan solve the limitations of a single deep model to a certain extent and improve the \ngeneralization ability and performance of the model. Tan et al. used model fusion \ntechnology to improve the performance of convolutional neural networks [19]. Mul-\ntimodal feature processing methods that exist tend to fall into two primary architec-\ntural categories: early fusion and late fusion. Early fusion refers to the integration \nthat occurs after the features are extracted, with the key advantage being the ability \nto leverage correlations between multiple features from different modalities at an \nearly stage. Late fusion, also known as decision-level fusion, refers to the process \nof integration that takes place after each modality has made its individual decision, \nwhether it be classification or regression.\n",
    "Research on multimodal hate speech detection based on…\nPage 5 of 17 \n   28 \nSai et al. used BERT for extracting text features and Inception v3, Inception \nResNet, and ResNext for extracting image features. They applied various early \nfusion techniques, such as concatenation and product rule, as well as late fusion \ntechniques including distribution summation, performance weighting, logarith-\nmic opinion polling, and rules learned from training on probabilities, to effec-\ntively integrate visual and textual modalities [20]. Liang Yi et al. were the first \nto employ a multi-layer CNN and a simple splicing method to fuse multimodal \nfeatures locally and globally. They input these two fusion features into differ-\nent classifiers to obtain two probability distributions and subsequently processed \nthe information through a hybrid fusion mode of late fusion by proportionally \nadding the two probability distributions [21]. Chuanpeng Yang et al. proposed \nan extensible cross-domain knowledge transfer (CDKT) framework, which can \nflexibly use mainstream visual language converters as the backbone [22]. Anu-\nsha Chhabra et al. introduced a “multi-scale Kernel attractive visual” (MSKAV) \nmodule, which can use an efficient multibranch structure to extract distinctive \nvisual features and perform post fusion by splicing the extracted multimodal \nfeatures [23]. Zheng Wenbo et al. embedded the text of multimodal knowledge \nmap attention image into the doctor–patient dialogs, achieving promising results \n[24]. Suyash Sangwan et al. used RNN and VGG-16 to extract text and image \nfeatures, and captured context information through a two-way gated loop unit to \nprocess multimodal data [25].\nResearch shows that although progress has been made in the detection of \nmultimodal hate speech, there is still a need for further investigation to address \nexisting problems and challenges. To our knowledge, the previous studies have \nnot employed the self-attention mechanism for the early fusion of image and \ntext modal features, which introduces a novel aspect to the methodology dis-\ncussed in Sect. 3. We believe that the application of self-attention mechanism in \nthis scenario has obvious advantages. It can capture more profound correlations \nbetween modalities, which is particularly critical for understanding and analyz-\ning hate speech that contains delicate emotions and implicit meanings. Com-\npared with the traditional feature fusion technology, the self-attention mecha-\nnism provides a more flexible and dynamic way to dynamically weight different \nmodalities’ information, to better reflect the complementarity and importance of \ndifferent modalities.\n3  \u0007Methodology\nIn the proposed method, we combine text and image bimodal features through a \nself-attention mechanism for hate speech detection. This method makes full use \nof the complementary effects of text and images to improve the performance of \nthe model on hate speech detection tasks to a certain extent. This section will \nintroduce our text and image processing methods and the method of applying \nmulti-head self-attention mechanism for feature fusion, respectively.\n",
    "\t\nJ. Mao et al.\n   28  \nPage 6 of 17\n3.1  \u0007Text modal processing\nRoBERTa, which stands for a robustly optimized BERT pretraining approach [26], \nis a natural language processing (NLP) pretraining model in the field of natural lan-\nguage processing (NLP) launched by Facebook AI in 2019. As compared to BERT \n(bidirectional encoder representations from transformers), RoBERTa utilizes a \nlarger corpus of text data for pretraining. This enhances the model’s ability to gen-\neralize. Moreover, RoBERTa incorporates a dynamic masking strategy that accom-\nmodates varying sentence lengths throughout the training process. This enables the \nmodel to capture a richer set of contextual information embedded within the text. \nIn this experiment, RoBERTa is employed to transform hate speech texts, which \nare extracted via OCR technology, into word vectors that serve as the text feature \ninputs, as shown in Eq. (1), where T is the input text content, and Vtxt ∈ℝn is the \nn-dimensional text feature vector. In our model, the dimensionality of the text fea-\ntures n is fixed to simplify model design and improve training efficiency. While the \ndimensionality of text features should typically be related to the length of the input \ntext, we adopt a fixed-dimensional approach to uniformly handle inputs of varying \nlengths. The input text length is variable, and during the preprocessing stage, we use \npadding and truncation techniques to ensure that all input samples have a consistent \nlength.\n3.2  \u0007Image model processing\n3.2.1  \u0007DeepFillV2\nDeepFillV2 is a deep learning-based image inpainting technique aimed at repairing \ndamaged, missing, or corrupted regions within an image by automatically filling in \nthe missing parts [27]. Its applications span image inpainting, photo enhancement, \nand other related fields. In the context of hate speech memes, characteristics such \nas the text’s location, length, and size can influence the model’s feature extraction \nprocess for the image. This paper adopts the approach presented in [28]. Initially, \nEasyOCR recognition technology is utilized to detect and extract the text’s loca-\ntion information, followed by the application of a segmentation mask. Subsequently, \nDeepFillV2 is employed to remove the text content from the meme image, resulting \nin clean, hateful memes without text.\n3.2.2  \u0007Swin transformer V2\nSwin transformer V2 is a deep learning model based on the self-attention mecha-\nnism launched by Microsoft Research Asia at the end of 2021. As the largest dense \nvision model to date, it boasts 3 billion parameters and can be trained on images \nwith resolutions up to 1536 × 1536 pixels [29]. This model draws inspiration from \n(1)\nVtxt = RoBERTa(CR(T))\n",
    "Research on multimodal hate speech detection based on…\nPage 7 of 17 \n   28 \nthe Vision transformer’s approach to processing images, utilizing a “res post” nor-\nmalization in place of the “pre” normalization configuration. It has set new per-\nformance benchmarks in four key visual tasks: ImageNet-V2 image classification, \nCOCO object detection, ADE20K semantic segmentation, and Kinetics-400 video \naction classification.\n3.2.3  \u0007Image feature extraction\nGiven that an image may contain text and other elements that could influence the \nmodel’s recognition capabilities, it is essential to initially eliminate the text from the \nimage data. To begin with, an image is inputted, and the image size is standardized \nto 224 × 224 pixels, as shown in Eq. (2).\nNext, assess whether the image currently being processed contains text content. If \ntext is present, proceed to input the image into the DeepFillV2 model for recognition \nand processing. Subsequently, remove the text-containing area from the image and \nperform the necessary repairs. Thereafter, utilize the Swin transformer V2 model \nto extract features from the image that has been fixed. This process is illustrated in \nEq. (3), where Vimg ∈ℝm is the m-dimensional image feature vector.\n3.3  \u0007Model fusion\nThe multi-head self-attention mechanism is a fundamental component widely uti-\nlized in natural language processing and neural networks as an attention mechanism. \nThis approach allows the model to concurrently focus on information from various \npositions within the input sequence, thereby enhancing its ability to capture long-\ndistance dependencies and associations. By assigning different attention weights to \ndifferent parts of the input, the model can selectively emphasize relevant features, \nwhich is crucial for tasks involving complex interactions between modalities. In our \nproposed framework, this mechanism plays a critical role in the fusion of text and \nimage feature vectors, as depicted in Fig. 2.\nSpecifically, the text and image features are first transformed into feature vectors, \ndenoted as V′\nimg and V′\ntxt , respectively, with matching dimensions through a dimen-\nsionality transformation. These vectors are then fed into the multi-head self-atten-\ntion module, which calculates the self-attention weights that quantify the importance \nof each text position in relation to the image positions, as depicted in Eq. (4).\nThe self-attention mechanism computes a set of attention scores, reflecting how \nmuch focus each text position should receive when analyzing a given image. This \n(2)\nIstd = resize(I, 224 × 224)\n(3)\nVimg = Swin transformer V2(Istd\n)\n(4)\nA = Multi head self attention\n(\nV\u001e\nimg, V\u001e\ntxt\n)\n",
    "\t\nJ. Mao et al.\n   28  \nPage 8 of 17\nability to weigh the importance of each textual element in relation to visual content \nenables the model to discern the semantic alignment between text and image more \neffectively.\nSubsequently, the text features are integrated with these weights through a dot-\nproduct operation, as illustrated in Eq. (5), to derive Vweight\ntxt\n . This step emphasizes \nthe textual components that exhibit a high correlation with the image features.\nThe intent here is to prioritize the text information that is most pertinent to \nthe image during the fusion process, thus enhancing the model’s classification \nperformance.\nFinally, the obtained text and image feature vectors are combined by splicing to \nform a new vector, Vfusion , representing the fused feature of both image and text. This \nimage–text fusion feature vector is then processed through a fully connected layer \nand a softmax layer, ultimately leading to the output layer of the model, where the \nfinal classification results are generated.\n4  \u0007Experiments\nThis paper conduct classification experiments using a multimodal hate speech data-\nset. In this section, we provide a brief overview of the experimental dataset, the vari-\nous evaluation metrics used to assess the classification outcomes, the configuration \nof the experimental environment, and the settings for hyperparameters.\n(5)\nVweight\ntxt\n= A ⋅V\u001e\ntxt\nFig. 2   Framework of the model\n",
    "Research on multimodal hate speech detection based on…\nPage 9 of 17 \n   28 \n4.1  \u0007Datasets\nHateful memes dataset [30] is a multimodal hate speech dataset that was devel-\noped by Facebook AI for the hateful memes challenge. Comprising 10,000 memes, \nthis dataset includes content posted on public social media platforms in the US. \nIt encompasses five distinct categories: multimodal hate, unimodal hate, benign \nimages, benign text, and non-hateful random content. Memes with hateful content \nconstitute 37% of the total dataset, enabling us to effectively evaluate the model’s \nability to identify minority class samples. Given its comprehensive nature and the \ndiversity of content types, the hateful memes dataset is an ideal choice for this \nresearch, allowing for focused exploration of hate speech detection within a well-\ndefined context.\n4.2  \u0007Classification metrics\nReferencing the evaluation indicators proposed by Chhabra et  al. for HSD tasks \n[31], this study employs a set of five distinct performance metrics, including accu-\nracy, recall, F1-score, and AUCROC, to conduct a comprehensive evaluation of the \nmodel’s performance. Table 1 presents the general formulation and the range of val-\nues for these performance metrics. An explanation of each performance metric is \nprovided below.\n1.\t Accuracy: Accuracy is a fundamental performance metric that measures the pro-\nportion of samples correctly classified by the model. It offers an overall assess-\nment of the model’s performance.\n2.\t Precision: Precision evaluates the model’s accuracy in predicting the positive \nclass, that is, the proportion of actual positive samples among all samples the \nmodel has predicted as positive.\n3.\t Recall: Recall measures the model’s ability to identify all positive class samples, \nwhich is the proportion of correctly predicted positive samples out of all actual \npositive samples.\n4.\t F1-score: The F1-score is the harmonic mean of precision and recall. It provides a \ncomprehensive performance metric that balances the trade-off between precision \nand recall.\n5.\t AUC-ROC (area under the receiver operating characteristic curve): AUC-ROC \nis an indicator used to measure the performance of a binary classification model. \nTable 1   General formula and \nscope for evaluating model \nclassification performance \nmetrics\nMetrics\nFormula\nRange\nAccuracy\nTP+TN\nTP+TN+FP+FN\n[0,1]\nPrecision\nTP\nTP+FP\n[0,1]\nRecall\nTP\nTP+FN\n[0,1]\nF1-score\n(2×recall×precision)\nrecall+precision\n[0,1]\nAUCROC\n∫ROC\n[0,1]\n",
    "\t\nJ. Mao et al.\n   28  \nPage 10 of 17\nIt is based on the area under the curve that plots the true-positive rate against the \nfalse-positive rate, offering a way to evaluate the model’s performance across \nvarious thresholds.\n4.3  \u0007Experimental environment configuration\nThe configuration of our experimental environment is shown in Table 2.\n4.4  \u0007Hyperparameter specification\nIn our experiments, we set the patch size for the Swin transformer to 4 × 4 pixels, \nthe dimension of embedding layer to 128, and the depth layer to [2,2,2,18]. For the \nnumber of attention heads used in the multi-head attention mechanism, we set it to \n[4,8,16,32], and the window size for moving is set to [8,8]. To mitigate the overfit-\nting problem that deep networks are prone to, the idea of random depth pruning is \nadopted, and the probability of closing each residual block is set to 0.5. In addition, \nwe used AdamW optimizer with a learning rate of 1e-5 and set the batch size to 64. \nThese parameters were tuned during the experiment, and we ran 30 epochs for train-\ning and selected the best model parameters based on the experimental results.\n5  \u0007Results\nThis section will present the accuracy, recall, F1-score, and AUCROC of our pro-\nposed multimodal joint model on the dataset and demonstrate the effectiveness of \nthis model by comparing its classification performance with those of single-mode \nmodels and the results from the previous research.\n5.1  \u0007Single‑modality comparison\nBy evaluating multiple models across different visual and textual modalities, our \ngoal is to identify the top-performing models in each modality for subsequent com-\nbination efforts. Analyzing the performance metrics of various models presented \nTable 2   Experimental \nenvironment configuration\nName\nVersion\nOperating system\nUbuntu 20.04.3\nCPU\nIntel(R) Xeon(R) \nGold 6240R CPU @ \n2.40 GHz\nGPU\nNVIDIA A100 80 GB\nPytorch\n1.13.1 + cu116\n",
    "Research on multimodal hate speech detection based on…\nPage 11 of 17 \n   28 \nin Table 3, we find that in the textual modality, XLNet performs notably in recall \n(0.6789). Meanwhile, RoBERTa stands out in the pure text hate speech classifica-\ntion task, achieving the highest scores for accuracy (0.7250), precision (0.6909), \nF1-score (0.6247), and AUCROC (0.6879). In the visual modality, ResNet shows \nsuperior performance in accuracy (0.8610) and precision (0.9763), Swin transformer \nhas the best recall (0.6789), and the second-generation Swin transformer excels in \nF1-score (0.7846) and AUCROC (0.8234).\nBased on the overall performance of each model, we select RoBERTa and Swin \ntransformer, which demonstrate excellence in their respective domains, for model \nfusion. This selection ensures that the combined model has the potential to achieve \noptimal performance.\n5.2  \u0007Baseline comparison\nOur research conducts a comprehensive comparison of the proposed model’s per-\nformance against leading image–text multimodal methods on the hateful memes \ndataset. Visual BERT COCO [30] is based on the BERT model structure, utilizes \na stacked transformer as an encoder for coding, and undergoes pretraining on \nthe COCO dataset. The DisMultiHate [32] model uses BERT and Faster R-CNN \nto extract text and image features and performs model fusion by concatenation. \nMSKAV [23] incorporates multiscale convolutional kernels, a multi-branch archi-\ntecture, and visual attention mechanisms for image feature extraction, and uses Dis-\ncolBERT, which is based on knowledge distillation, for text feature extraction. The \nfeature fusion in this model is carried out using a serial approach.\nTable 4 presents the results indicating that our proposed multimodal hate speech \njoint model, which integrates multi-level visual features using moving windows and \nemploys text feature extraction based on the RoBERTa pretraining model through \na multi-head self-attention mechanism, has achieved notable performance improve-\nments on the hateful memes dataset. Specifically, there was an enhancement of 0.3% \nin accuracy and 8.02% in precision. While the increase in accuracy may appear \nmodest, it is essential to consider that this small gain could significantly impact real-\nworld applications, particularly in reducing false negatives in hate speech detection. \nTable 3   Comparison table of performance indicators of each model in visual and text modes\nThe bold value indicates optimal performance values of each indicator\nModal\nModel\nAccuracy\nPrecision\nRecall\nF1-score\nAUCROC\nText\nXLNet\n0.6900\n0.5785\n0.6789\n0.5802\n0.6815\nALBERT\n0.7020\n0.6331\n0.5132\n0.5669\n0.6654\nBERT\n0.6990\n0.6082\n0.5842\n0.5960\n0.6768\nRoBERTa\n0.7250\n0.6909\n0.5237\n0.6247\n0.6879\nImage\nViT\n0.8481\n0.9713\n0.6236\n0.7596\n0.8061\nResNet\n0.8610\n0.9763\n0.6500\n0.7804\n0.8202\nSwin transformer\n0.8520\n0.9084\n0.6789\n0.7771\n0.8185\nSwin transformer V2\n0.8600\n0.9444\n0.6711\n0.7846\n0.8234\n",
    "\t\nJ. Mao et al.\n   28  \nPage 12 of 17\nMoreover, our model showed a significant increase of 12.87% in the F1-score met-\nric, suggesting that our approach has bettered the balance between accuracy and \nrecall for the task of detecting hateful memes. In addition, the AUCROC metric saw \nan improvement of 1.49%, demonstrating our model’s enhanced capability to distin-\nguish between positive and negative samples in the classification of hateful memes, \nfurther underscoring the practical utility of our model in complex scenarios.\nTable  5 provides an example of the model’s judgment results concerning the \npresence of hate factors in the test set data. The text within the images is accurately \nrecognized and extracted, and the areas originally covered by the text are effectively \neliminated from the images. The classification results demonstrate that the model \nTable 4   Comparison table of benchmark dataset performance indicators\nThe bold value indicates optimal performance values of each indicator\nModel\nAccuracy\nPrecision\nRecall\nF1-score\nAUCROC\nVisual BERT COCO\n0.6947\n–\n–\n–\n0.7544\nDisMultiHate\n0.7580\n–\n–\n–\n0.8280\nMSKAV\n0.8750\n0.8333\n–\n0.6950\n0.8383\nOur model\n0.8780\n0.9135\n0.7500\n0.8237\n0.8532\nTable 5   Example of some results in the test set\nImage\nFixed image\nText\nResult\nHappy pride month let’s go beat \nup lesbians\nHate\nHow to fix a noisy dishwasher\nHate\nOrganic vegetable\nNo hate\n",
    "Research on multimodal hate speech detection based on…\nPage 13 of 17 \n   28 \nhas learned to discern the implicit associations between text and image. Particu-\nlarly with the image captioned “how to fix a noisy dishwasher,” the model exhibits \nadvanced cognitive capabilities in understanding the relationship between text and \nimage contents. It identifies the discrepancy between the potentially misleading text \nand the innocuous nature of the image itself, indicating that the model transcends \nmere surface-level recognition to interpret the underlying emotions and intentions.\n5.3  \u0007Ablation trials\nThis section presents the results of ablation experiments conducted to assess the \neffectiveness of the proposed model on the hateful memes dataset. Within the frame-\nwork of the proposed multimodal joint model, three distinct ablation studies were \ncarried out. The first study modeled and analyzed the textual component exclusively. \nThe second investigation was dedicated to confirming the model’s efficacy with \nrespect to the image component alone. The third experiment entailed the removal of \nthe self-attention mechanism used for feature fusion. The outcomes of these ablation \nexperiments are detailed in Table 6.\nThe results of the ablation experiment clearly indicate that our multimodal hate \nspeech detection joint model has a broader perspective in content understanding and \nis significantly better than the single-modal model. In the experiment using only text \ncomponents, the accuracy of the model was 0.7250, indicating the importance of \ntextual information. In the experiment focused on image components, the accuracy \nof the model was 0.8600. Although it performed well, it failed to fully capture the \npotential hate meanings in the text. These results indicate that in situations involving \nambiguity or puns, the model can more accurately identify true intentions and emo-\ntional colors by integrating visual and linguistic cues.\nIn addition, when there is noise or misleading information in one mode, the mul-\ntimodal model can use the information of another mode to modify, to improve its \nrobustness. The self-attention mechanism used in the model effectively captures \nlong-distance dependencies and dynamically adjusts the attention weights of differ-\nent modal information, making the integration process of multimodal information \nmore efficient and flexible. This integration method significantly enhances the over-\nall performance of the model, rendering the fusion process more agile and efficient.\nBased on the observations from the ablation experiments, the following conclu-\nsions can be drawn:\nTable 6   Comparison table of ablation experiment results\nThe bold value indicates optimal performance values of each indicator\nModel\nAccuracy\nPrecision\nRecall\nF1-score\nAUCROC\nRoBERTa\n0.7250\n0.6909\n0.5237\n0.5802\n0.6815\nSwin transformer V2\n0.8600\n0.9444\n0.6711\n0.7846\n0.8234\nModel/no attention\n0.8620\n0.8104\n0.8477\n0.7763\n0.8454\nOur model\n0.8780\n0.9135\n0.7500\n0.8237\n0.8532\n",
    "\t\nJ. Mao et al.\n   28  \nPage 14 of 17\n1.\t Our proposed multimodal hate speech detection joint model performs best on the \nhateful memes dataset.\n2.\t Removing any modality leads to a deterioration in the model’s predictive perfor-\nmance.\n3.\t Using multi-head self-attention mechanism for fusing textual and visual features \neffectively enhances the overall performance of the model.\n5.4  \u0007Engineering applications\nIn this section, we discuss the potential value and practical feasibility of the pro-\nposed multimodal hate speech detection joint model. Our model performs well on \nthe hateful memes dataset, which is significantly improved compared with the cur-\nrent leading methods. This highlights its ability to identify and classify texts and \nimages containing hate factors and provides strong support for applications in social \nmedia platforms, news websites, and online communities. However, since the model \nis only tested on open datasets, its generalization ability on other datasets has not yet \nbeen evaluated.\nIn addition, by comparing the results of single-mode and multi-mode and abla-\ntion experiments, we confirmed the advantages of the model in integrating text and \nvisual information, which is particularly important in scenarios where multiple data \nsources need to be synthesized for decision-making, such as social media content \nreview and news report authenticity verification. In application scenarios such as the \nmedical field, understanding the decision-making process of the model is particu-\nlarly critical. Therefore, our model adopts the self-attention mechanism for feature \nfusion. Although the overall performance of the model has been improved, the inter-\npretability still needs further research.\nIn general, although our multimodal hate speech detection joint model shows \ngood application potential on the current dataset, future research should focus on \nverifying its performance on different datasets to ensure its effectiveness and reli-\nability in practical applications.\n6  \u0007Conclusions\nThis article presents a novel multimodal hate speech detection model that combines \nRoBERTa for text processing and Swin transformer V2 for image feature extraction \nusing a multi-head self-attention mechanism. Trained on a public multimodal hate \nspeech dataset, the model demonstrates superior performance compared to baseline \nmodels, achieving strong results in accuracy, precision, recall, F1-score, and AUC-\nROC. While statistical significance tests indicate that some improvements may not \nmeet conventional thresholds (p > 0.05), ablation experiments confirm the model’s \neffectiveness in enhancing hate speech detection.\nHowever, challenges remain, particularly with sarcasm and ambiguous visual \ncues, which can lead to misclassification, revealing limitations in the model’s under-\nstanding of nuanced meanings. In summary, our research introduces an effective \n",
    "Research on multimodal hate speech detection based on…\nPage 15 of 17 \n   28 \nmultimodal approach that fuses textual and visual information, offering valuable \ninsights for hate speech detection systems on social media platforms.\nDespite the promising results, we acknowledge limitations in interpretability, as \nthe model’s decision-making process may lack transparency. Future research should \nfocus on developing advanced interpretive AI technologies to improve model trans-\nparency and broaden its applicability.\nAuthor contributions  Mao wrote the main manuscript text, and Shi implemented the image model \nprocessing and related experiments. Li prepared Figs. 1 and 2 and Tables 1, 2, 3, 4, and 5. All authors \nreviewed the manuscript.\nData availability  No datasets were generated or analyzed during the current study.\nDeclarations \nConflict of interest  The authors declare no competing interests.\nReferences\n\t 1.\t Gomez R, Gibert J, Gomez L, Karatzas D (2020) Exploring hate speech detection in multimodal \npublications. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer \nVision, pp 1470–1478. https://​doi.​org/​10.​1109/​WACV4​5572.​2020.​90934​14\n\t 2.\t Guterres A (2019) United Nations strategy and plan of action on hate speech. United Nations. \nhttps://​www.​un.​org/​en/​hate-​speech/​un-​strat​egy-​and-​plan-​of-​action-​on-​hate-​speech\n\t 3.\t Mora D (2022) New UN study finds that hate speech in Costa Rica grew by 71%. https://​unsdg.​un.​\norg/​latest/​stori​es/​new-​un-​study-​finds-​hate-​speech-​costa-​rica-​grew-​71\n\t 4.\t Yadav A, Chandel S, Chatufale S, Bandhakavi A (2023) Lahm: large annotated dataset for multi-\ndomain and multilingual hate speech identification. arXiv preprint arXiv:​2304.​00913. https://​arxiv.​\norg/​pdf/​2304.​00913\n\t 5.\t Leite JA, Scarton C, Silva DF (2023) Noisy self-training with data augmentations for offensive and \nhate speech detection tasks. arXiv preprint arXiv:​2307.​16609. https://​arxiv.​org/​pdf/​2307.​16609\n\t 6.\t Tontodimamma A, Nissi E, Sarra A, Fontanella L (2021) Thirty years of research into hate speech: \ntopics of interest and their evolution. Scientometrics 126:157–179. https://​doi.​org/​10.​1007/​\ns11192-​020-​03737-6\n\t 7.\t Wang J, Wang S, Lin M, Xu Z, Guo W (2023) Learning speaker-independent multimodal represen-\ntation for sentiment analysis. Inf Sci 628:208–225. https://​doi.​org/​10.​1016/j.​ins.​2023.​01.​116\n\t 8.\t Capozzi Arthur TE, Mirko L, Valerio B, Fabio P, Sanguinetti M, Cristina B, Marco S (2019) Com-\nputational linguistics against hate: hate speech detection and visualization on social media in the \n“Contro L’Odio” project. In: CEUR workshop proceedings, vol 2481. CEUR-WS. https://​iris.​unica.​\nit/​bitst​ream/​11584/​389784/​1/​clic2​019_​hs-​cl.​pdf\n\t 9.\t Basile V, Bosco C, Fersini E, Nozza D, Patti V, Pardo FMR, Sanguinetti M (2019) Semeval-2019 \ntask 5: multilingual detection of hate speech against immigrants and women in twitter. In: Proceed-\nings of the 13th international workshop on semantic evaluation, pp 54–63. https://​aclan​tholo​gy.​org/​\nS19-​2007.​pdf\n\t10.\t Florio K, Basile V, Lai M, Patti V (2019). Leveraging hate speech detection to investigate immi-\ngration-related phenomena in Italy. In: 2019 8th International Conference on Affective Computing \nand Intelligent Interaction Workshops and Demos (ACIIW), pp 1–7. IEEE. https://​doi.​org/​10.​1109/​\nACIIW.​2019.​89250​79\n\t11.\t Mozafari M, Farahbakhsh R, Crespi N (2020) Hate speech detection and racial bias mitigation in \nsocial media based on BERT model. PLoS ONE 15(8):e0237861. https://​doi.​org/​10.​1371/​journ​al.​\npone.​02378​61\n",
    "\t\nJ. Mao et al.\n   28  \nPage 16 of 17\n\t12.\t Uyheng J, Bellutta D, Carley KM (2022) Bots amplify and redirect hate speech in online discourse \nabout racism during the COVID-19 pandemic. Soc Media Soc 8(3):20563051221104748. https://​\ndoi.​org/​10.​1177/​20563​05122​11047​49\n\t13.\t Plaza-del-Arco FM, Molina-González MD, Urena-López LA, Martín-Valdivia MT (2021) Compar-\ning pre-trained language models for Spanish hate speech detection. Expert Syst Appl 166:114120. \nhttps://​doi.​org/​10.​1016/j.​eswa.​2020.​114120\n\t14.\t Akuma S, Lubem T, Adom IT (2022) Comparing bag of words and TF-IDF with different models \nfor hate speech detection from live tweets. Int J Inf Technol 14(7):3629–3635. https://​doi.​org/​10.​\n1007/​s41870-​022-​01096-4\n\t15.\t Alkomah F, Ma X (2022) A literature review of textual hate speech detection methods and datasets. \nInformation 13(6):273. https://​doi.​org/​10.​3390/​info1​30602​73\n\t16.\t MacAvaney S, Yao HR, Yang E, Russell K, Goharian N, Frieder O (2019) Hate speech detection: \nchallenges and solutions. PLoS ONE 14(8):e0221152. https://​doi.​org/​10.​1371/​journ​al.​pone.​02211​\n52\n\t17.\t Gambäck B, Sikdar UK (2017). Using convolutional neural networks to classify hate-speech. In: \nProceedings of the first workshop on abusive language online, pp 85–90. https://​doi.​org/​10.​18653/​\nv1/​W17-​3013\n\t18.\t Alshalan R, Al-Khalifa H (2020) A deep learning approach for automatic hate speech detection in \nthe Saudi twittersphere. Appl Sci 10(23):8614. https://​doi.​org/​10.​3390/​app10​238614\n\t19.\t Tan M (2019) Efficientnet: rethinking model scaling for convolutional neural networks. arXiv pre-\nprint arXiv:​1905.​11946. https://​rxiv.​org/​abs/​1905.​11946\n\t20.\t Sai S, Srivastava ND, Sharma Y (2022) Explorative application of fusion techniques for multimodal \nhate speech detection. SN Comput Sci 3(2):122. https://​doi.​org/​10.​1007/​s42979-​021-​01007-7\n\t21.\t Liang Yi T, Tuerdi T (2023) Multimodal fake information detection using multi-layer CNN feature \nfusion and multi-classifier ensemble prediction. Comput Eng Sci https://​kns.​cnki.​net/​kcms2/​artic​\nle/​abstr​act?v=​bh5BU​r5hMu​VOZ89​CJ0yy​4UtbD​D72fK​vyRiR​9xyB-​TIpsal-​6RVRd​hWdot​CyBZq​\nex1qR_​gPy_​jObrq​s1QtO​6Vbjt​Munpi​fN6MC​vW958​N07dm​aUt_​uZpuM​k2-​b6nA1​hOHLZ​ORmUf​\npsPqk​J2muS​9Wqvw​lr6wV​v63mx​WCdJH​Zg7UM--​JuwEd​OmiuX-​gF3y6X_​BmORk​Spz1J​mBoM=​\n&​unipl​atform=​NZKPT​&​langu​age=​CHS\n\t22.\t Yang C, Zhu F, Liu G, Han J, Hu S (2022). Multimodal hate speech detection via cross-domain \nknowledge transfer. In: Proceedings of the 30th ACM International Conference on Multimedia, pp \n4505–4514. https://​doi.​org/​10.​1145/​35031​61.​35482​55\n\t23.\t Chhabra A, Vishwakarma DK (2023) Multimodal hate speech detection via multi-scale visual ker-\nnels and knowledge distillation architecture. Eng Appl Artif Intell 126:106991. https://​doi.​org/​10.​\n1016/j.​engap​pai.​2023.​106991\n\t24.\t Zheng W, Yan L, Gou C, Zhang ZC, Zhang JJ, Hu M, Wang FY (2021) Pay attention to doctor–\npatient dialogues: multi-modal knowledge graph attention image-text embedding for COVID-19 \ndiagnosis. Inform Fus 75:168–185. https://​doi.​org/​10.​1016/j.​inffus.​2021.​05.​015\n\t25.\t Sangwan S, Akhtar MS, Behera P, Ekbal A (2020). I didn’t mean what I wrote! exploring multimo-\ndality for sarcasm detection. In: 2020 International Joint Conference on Neural Networks (IJCNN), \npp 1–8. IEEE. https://​doi.​org/​10.​1109/​IJCNN​48605.​2020.​92069​05\n\t26.\t Liu Y (2019) Roberta: a robustly optimized bert pretraining approach. arXiv preprint arXiv:​1907.​\n11692. https://​arxiv.​org/​abs/​1907.​11692\n\t27.\t Yu J, Lin Z, Yang J, Shen X, Lu X, Huang TS (2019) Free-form image inpainting with gated convo-\nlution. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp 4471–\n4480. https://​doi.​org/​10.​1109/​ICCV.​2019.​00457\n\t28.\t Zhu R (2020) Enhance multimodal transformer with external label and in-domain pretrain: hate-\nful meme challenge winning solution. arXiv preprint arXiv:​2012.​08290. https://​arxiv.​org/​abs/​2012.​\n08290\n\t29.\t Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Guo B (2021) Swin transformer: Hierarchical vision \ntransformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on \nComputer Vision, pp 10012–10022). https://​doi.​org/​10.​1109/​ICCV4​8922.​2021.​00986\n\t30.\t Kiela D, Firooz H, Mohan A, Goswami V, Singh A, Ringshia P, Testuggine D (2020) The hateful \nmemes challenge: detecting hate speech in multimodal memes. Adv Neural Inform Process Syst \n33:2611–2624\n\t31.\t Chhabra A, Vishwakarma DK (2023) A literature survey on multimodal and multilingual \nautomatic hate speech identification. Multim Syst 29(3):1203–1230. https://​doi.​org/​10.​1007/​\ns00530-​023-​01051-8\n",
    "Research on multimodal hate speech detection based on…\nPage 17 of 17 \n   28 \n\t32.\t Lee RKW, Cao R, Fan Z, Jiang J, Chong WH (2021). Disentangling hate in online memes. In: Pro-\nceedings of the 29th ACM International Conference on Multimedia, pp 5138–5147. https://​doi.​org/​\n10.​1145/​34740​85.​347562\nPublisher’s Note  Springer Nature remains neutral with regard to jurisdictional claims in published maps \nand institutional affiliations.\nSpringer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under \na publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted \nmanuscript version of this article is solely governed by the terms of such publishing agreement and \napplicable law.\n"
  ]
}