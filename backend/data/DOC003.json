{
  "doc_id": "DOC003",
  "filename": "Wasserstoff Gen-AI Internship Task.pdf",
  "extraction_method": "pdf-text",
  "upload_time": "2025-06-01T09:19:22Z",
  "content": [
    "AI Software Intern – Internship Task Document \nInternship Role Overview \nAs an AI Intern for 6 months (full-time), you will engage in research-driven development \nof Generative AI applications. The internship emphasizes both academic research and \nhands-on implementation, contributing to real product development, exploring research \npapers, and building internal tools. \nCompany: Wasserstoff \nRole: AI Intern (Generative AI) – Full-Time, 6 Month Internship \nFocus: Research-based implementation of Generative AI tools and applications (mix of \nresearch and real product development) \nRequired Skills: Python; Transformers and LLM architecture; LangChain framework; \nOpenAI API usage; system design fundamentals; basic database knowledge \n(SQL/NoSQL) \n \nInternship Task: Document Research & Theme Identification Chatbot \nObjective: \nCreate an interactive chatbot that can perform research across a large set of documents \n(minimum 75 documents), identify common themes (multiple themes are possible), and \nprovide detailed, cited responses to user queries. \n \nTask Breakdown: \n1. Document Upload and Knowledge Base Creation: \n●​ Allow users to upload 75+ documents in various formats including PDF and \nscanned images.​\n \n●​ Convert and preprocess scanned documents using OCR (Optical Character \nRecognition).​\n \n",
    "●​ Extract text content accurately, ensuring high fidelity for research purposes.​\n \n●​ Integrate and store uploaded documents in a database for reuse.​\n \n2. Document Management & Query Processing: \n●​ Develop an intuitive interface where users can view all uploaded documents.​\n \n●​ Allow users to input queries in natural language.​\n \n●​ Process these queries individually against each document.​\n \n●​ Extract relevant responses with precise citations clearly indicating locations \n(page, paragraph, sentence).​\n \n3. Theme Identification & Cross-Document Synthesis: \n●​ Analyze responses from all documents collectively.​\n \n●​ Identify coherent common themes across the documents (multiple themes \npossible).​\n \n●​ Produce a final synthesized answer clearly indicating all identified themes.​\n \n●​ Provide comprehensive citation mapping at a minimum document-level \ngranularity to support each synthesized theme.​\n \n \nAdditional Functionalities (Extra Credit): \n●​ Enhanced granularity of citations (paragraph or sentence level).​\n \n●​ Visual representation or mapping interface linking citations to documents.​\n \n●​ Advanced filtering options (date, author, document type, relevance scores).​\n \n●​ Enable selection/deselection of specific documents for targeted querying.​\n \n",
    " \nTechnical Requirements: \n●​ Modern AI language models (OpenAI GPT, Gemini, Groq) .  \n❖​ GPT and Gemini give free credits for testing and project purposes \n❖​ Groq is a free LLM service hosting LLAMA.​\n \n●​ Vector databases (Qdrant, ChromaDB, FAISS) for efficient semantic search.​\n \n●​ OCR libraries (Tesseract, PaddleOCR) for scanned documents.​\n \n●​ Python frameworks such as FastAPI or Flask for backend.​\n \n●​ Deployment on free hosting services if necessary (see deployment links below).​\n \n \nDeliverables: \n●​ Fully functional web-based chatbot with documented code.​\n \n●​ Brief report explaining methodologies and technologies used.​\n \n●​ Demonstration video or presentation showcasing functionality and results.​\n \n \nEvaluation Criteria: \n●​ Functionality: Objectives achievement of document research, theme \nidentification, citation.​\n \n●​ Code Quality & Structure: Modular design, readability, proper naming, \ncomments, version control usage.​\n \n●​ Error Handling: Robust handling of exceptions or failures.​\n \n●​ Documentation Clarity: Comprehensive README and demonstration video.​\n \n",
    "●​ System Design Insight: Scalability and deployment considerations.​\n \n●​ User Interface: Simplicity and clarity.​\n \n \nPresentation of Results: \n●​ Individual document responses in tabular format. Example:​\n \nDocument \nID \nExtracted Answer \nCitation \nDOC001 \nThe order states that the fine was imposed under section \n15 of the SEBI Act. \nPage 4, \nPara 2 \nDOC002 \nTribunal observed delay in disclosure violated Clause 49 \nof LODR. \nPage 2, \nPara 1 \n●​ ​\nFinal synthesized response in chat format with clear citations marked by \ndocument IDs. Example:​\n \nTheme 1 – Regulatory Non-Compliance:​\n Documents (DOC001, DOC002) highlight regulatory non-compliance with \nSEBI Act and LODR. \nTheme 2 – Penalty Justification:​\n DOC001 explicitly justifies penalties under statutory frameworks. \n \nRecommended Folder Structure: \nchatbot_theme_identifier/ \n├── backend/ \n│   ├── app/ \n│   │   ├── api/ \n│   │   ├── core/ \n│   │   ├── models/ \n│   │   ├── services/ \n",
    "│   │   ├── main.py \n│   │   └── config.py \n│   ├── data/ \n│   ├── Dockerfile \n│   └── requirements.txt \n├── docs/ \n├── tests/ \n├── demo/ \n└── README.md \n \n \nDataset Notes: \nThe dataset can be any set of semantically related documents from domains such as \nlegal case orders, technical reports, business documents, medical research, or policy \npapers. \n \nFree Deployment Platforms: \n●​ Render​\n \n●​ Railway​\n \n●​ Replit​\n \n●​ Hugging Face Spaces​\n \n●​ Vercel​\n \n \nContact \nFor any questions during the internship or to submit your deliverables, please reach out \nto: \nDivyansh Sharma – Wasserstoff​\n Email: divyansh.sharma@thewasserstoff.com \n"
  ]
}